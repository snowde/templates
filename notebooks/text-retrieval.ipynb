{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text retrieval\n",
    "\n",
    "This guide will introduce techniques for organizing text data. It will show how to analyze a large corpus of text, extracting feature vectors for individual documents, in order to be able to retrieve documents with similar content.\n",
    "\n",
    "[scipy](https://www.scipy.org/) and [scikit-learn](scikit-learn.org) are required to run through this document, as well as a corpus of text documents. This code can be adapted to work with a set of documents you collect. For the purpose of this example, we will use the well-known Reuters-21578 dataset with 90 categories. To download this dataset, download it from [here](http://disi.unitn.it/moschitti/corpora.htm) or run:\n",
    "\n",
    "    wget http://disi.unitn.it/moschitti/corpora/Reuters21578-Apte-90Cat.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've downloaded and unzipped the dataset, take a look inside the folder. It is split into two folders, \"training\" and \"test\". Each of those contains 91 subfolders, corresponding to pre-labeled categories, which will be useful for us later when we want to try classifying the category of an unknown message. In this notebook, we are not worried about training a classifier, so we'll end up using both sets together.\n",
    "\n",
    "Let's note the location of the folder into a variable `data_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/Reuters21578-Apte-90Cat/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open up a single message and look at the contents. This is the very first message in the training folder, inside of the \"acq\" folder, which is a category apparently containing news of corporate acquisitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\n",
      "COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE\r\n",
      "\n",
      "     COMMACK, N.Y., Feb 26 - Computer Terminal Systems Inc said\r\n",
      "it has completed the sale of 200,000 shares of its common\r\n",
      "stock, and warrants to acquire an additional one mln shares, to\r\n",
      "<Sedio N.V.> of Lugano, Switzerland for 50,000 dlrs.\r\n",
      "    The company said the warrants are exercisable for five\r\n",
      "years at a purchase price of .125 dlrs per share.\r\n",
      "    Computer Terminal said Sedio also has the right to buy\r\n",
      "additional shares and increase its total holdings up to 40 pct\r\n",
      "of the Computer Terminal's outstanding common stock under\r\n",
      "certain circumstances involving change of control at the\r\n",
      "company.\r\n",
      "    The company said if the conditions occur the warrants would\r\n",
      "be exercisable at a price equal to 75 pct of its common stock's\r\n",
      "market price at the time, not to exceed 1.50 dlrs per share.\r\n",
      "    Computer Terminal also said it sold the technolgy rights to\r\n",
      "its Dot Matrix impact technology, including any future\r\n",
      "improvements, to <Woodco Inc> of Houston, Tex. for 200,000\r\n",
      "dlrs. But, it said it would continue to be the exclusive\r\n",
      "worldwide licensee of the technology for Woodco.\r\n",
      "    The company said the moves were part of its reorganization\r\n",
      "plan and would help pay current operation costs and ensure\r\n",
      "product delivery.\r\n",
      "    Computer Terminal makes computer generated labels, forms,\r\n",
      "tags and ticket printers and terminals.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "post_path = os.path.join(data_dir, \"training\", \"acq\", \"0000005\")\n",
    "with open (post_path, \"r\") as p:\n",
    "    raw_text = p.read()\n",
    "    print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our collection contains over 15,000 articles with a lot of information. It would take way too long to get through all the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'unknown', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "# this gives us all the groups (from training subfolder, but same for test)\n",
    "groups = [g for g in os.listdir(os.path.join(data_dir, \"training\")) if os.path.isdir(os.path.join(data_dir, \"training\", g))]\n",
    "print groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all of our documents (news articles) into a single list called `docs`. We'll iterate through each group, grab all of the posts in each group (from both training and test directories), and add the text of the post into the `docs` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading group 1 / 91\n",
      "reading group 11 / 91\n",
      "reading group 21 / 91\n",
      "reading group 31 / 91\n",
      "reading group 41 / 91\n",
      "reading group 51 / 91\n",
      "reading group 61 / 91\n",
      "reading group 71 / 91\n",
      "reading group 81 / 91\n",
      "reading group 91 / 91\n",
      "\n",
      "we have 15437 documents in 91 groups\n",
      "\n",
      "here is document 100:\n",
      "\n",
      "\n",
      "ULTRAMAR SELLS U.K. MARKETING UNITS FOR 50 MLN STG\n",
      "\n",
      "    LONDON, March 2 - Ultramar Plc <UMAR.L> said it had reached\n",
      "agreement in principle to sell its wholly owned U.K. Marketing\n",
      "companies to Kuwait Petroleum Corp for around 50 mln stg.\n",
      "    Ultramar's marketing units include <Ultramar Golden Eagle\n",
      "Ltd> which in 1985 made a profit of around 1.4 mln stg before\n",
      "financing and group administration charges. A small loss was\n",
      "recorded for the first nine months of 1986.\n",
      "    The sale is due to take place on April 1 with the proceeds\n",
      "intended to reduce group debt in the short term. But Ultramar\n",
      "said the funds would ultimately be used for further development\n",
      "of its core businesses in the U.K. And North America.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "docs = []\n",
    "for g, group in enumerate(groups):\n",
    "    if g%10==0:\n",
    "        print (\"reading group %d / %d\"%(g+1, len(groups)))\n",
    "    posts_training = [os.path.join(data_dir, \"training\", group, p) for p in os.listdir(os.path.join(data_dir, \"training\", group)) if os.path.isfile(os.path.join(data_dir, \"training\", group, p))]\n",
    "    posts_test = [os.path.join(data_dir, \"test\", group, p) for p in os.listdir(os.path.join(data_dir, \"test\", group)) if os.path.isfile(os.path.join(data_dir, \"test\", group, p))]\n",
    "    posts = posts_training + posts_test\n",
    "    for post in posts:\n",
    "        with open(post, \"r\") as p:\n",
    "            raw_text = p.read()\n",
    "            raw_text = re.sub(r'[^\\x00-\\x7f]',r'', raw_text) \n",
    "            docs.append(raw_text)\n",
    "\n",
    "print(\"\\nwe have %d documents in %d groups\"%(len(docs), len(groups)))\n",
    "print(\"\\nhere is document 100:\\n%s\"%docs[100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use `sklearn`'s `TfidfVectorizer` to compute the tf-idf matrix of our collection of documents. The tf-idf matrix is an `n`x`m` matrix with the `n` rows corresponding to our `n` documents and the `m` columns corresponding to our terms. The values corresponds to the \"importance\" of each term to each document, where importance is *. In this case, terms are just all the unique words in the corpus, minus english _stopwords_, which are the most common words in the english language, e.g. \"it\", \"they\", \"and\", \"a\", etc. In some cases, terms can be n-grams (n-length sequences of words) or more complex, but usually just words.\n",
    "\n",
    "To compute our tf-idf matrix, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15437x33973 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 972142 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(docs)\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variable `tfidf` is a sparse matrix with a row for each document, and a column for each unique term in the corpus. \n",
    "\n",
    "Thus, we can interpret each row of this matrix as a feature vector which describes a document. Two documents which have identical rows have the same collection of words in them, although not necessarily in the same order; word order is not preserved in the tf-idf matrix. Regardless, it seems reasonable to expect that if two documents have similar or close tf-idf vectors, they probably have similar content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\n",
      "INVESTMENT FIRMS CUT CYCLOPS <CYL> STAKE\r\n",
      "\n",
      "    WASHINGTON, Feb 26 - A group of affiliated New York\r\n",
      "investment firms said they lowered their stake in Cyclops Corp\r\n",
      "to 260,500 shares, or 6.4 pct of the total outstanding common\r\n",
      "stock, from 370,500 shares, or 9.2 pct.\r\n",
      "    In a filing with the Securities and Exchange Commission,\r\n",
      "the group, led by Mutual Shares Corp, said it sold 110,000\r\n",
      "Cyclops common shares on Feb 17 and 19 for 10.0 mln dlrs.\r\n",
      "\r\n",
      "document's term-frequency pairs:\n",
      "\"17\"=0.08, \"110\"=0.14, \"mutual\"=0.16, \"led\"=0.11, \"commission\"=0.10, \"exchange\"=0.08, \"securities\"=0.09, \"filing\"=0.13, \"370\"=0.16, \"500\"=0.19, \"260\"=0.15, \"lowered\"=0.14, \"affiliated\"=0.17, \"washington\"=0.07, \"stake\"=0.21, \"cyl\"=0.18, \"cyclops\"=0.52, \"cut\"=0.09, \"firms\"=0.24, \"investment\"=0.19, \"group\"=0.16, \"19\"=0.08, \"10\"=0.07, \"york\"=0.07, \"new\"=0.06, \"corp\"=0.12, \"sold\"=0.10, \"outstanding\"=0.10, \"pct\"=0.10, \"total\"=0.08, \"dlrs\"=0.05, \"mln\"=0.04, \"stock\"=0.08, \"common\"=0.19, \"shares\"=0.33, \"000\"=0.06, \"said\"=0.07, \"26\"=0.09, \"feb\"=0.21\n"
     ]
    }
   ],
   "source": [
    "doc_idx = 5\n",
    "\n",
    "doc_tfidf = tfidf.getrow(doc_idx)\n",
    "all_terms = vectorizer.get_feature_names()\n",
    "terms = [all_terms[i] for i in doc_tfidf.indices]\n",
    "values = doc_tfidf.data\n",
    "\n",
    "print(docs[doc_idx])\n",
    "print(\"document's term-frequency pairs:\")\n",
    "print(\", \".join(\"\\\"%s\\\"=%0.2f\"%(t,v) for t,v in zip(terms,values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice however, the term-document matrix alone has several disadvantages. For one, it is very high-dimensional and sparse (mostly zeroes), thus it is computationally costly. \n",
    "\n",
    "Additionally, it ignores similarity among groups of terms. For example, the words \"seat\" and \"chair\" are related, but in a raw term-document matrix they are separate columns. So two sentences with one of each word will not be computed as similarly.\n",
    "\n",
    "One solution is to use latent semantic analysis (LSA, or sometimes called latent semantic indexing). LSA is a dimensionality reduction technique closely related to principal component analysis, which is commonly used to reduce a high-dimensional set of terms into a lower-dimensional set of \"concepts\" or components which are linear combinations of the terms.\n",
    "\n",
    "To do so, we use `sklearn`'s `TruncatedSVD` function which gives us the LSA by computing a [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) of the tf-idf matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa = TruncatedSVD(n_components=100)\n",
    "tfidf_lsa = lsa.fit_transform(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret this? `lsa` holds our latent semantic analysis, expressing our 100 concepts. It has a vector for each concept, which holds the weight of each term to that concept. `tfidf_lsa` is our transformed document matrix where each document is a weighted sum of the concepts. \n",
    "\n",
    "In a simpler analysis with, for example, two topics (sports and tacos), one concept might assign high weights for sports-related terms (ball, score, tournament) and the other one might have high weights for taco-related concepts (cheese, tomato, lettuce). In a more complex one like this one, the concepts may not be as interpretable. Nevertheless, we can investigate the weights for each concept, and look at the top-weighted ones. For example, here are the top terms in concept 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 highest-weighted terms in concept 1:\n",
      " - vs : 32800.00\n",
      " - net : 21097.00\n",
      " - loss : 18711.00\n",
      " - cts : 8738.00\n",
      " - 000 : 1.00\n",
      " - revs : 26258.00\n",
      " - shr : 28000.00\n",
      " - profit : 24258.00\n",
      " - avg : 4002.00\n",
      " - shrs : 28015.00\n"
     ]
    }
   ],
   "source": [
    "components = lsa.components_[1]\n",
    "all_terms = vectorizer.get_feature_names()\n",
    "\n",
    "idx_top_terms = sorted(range(len(components)), key=lambda k: components[k])\n",
    "\n",
    "print(\"10 highest-weighted terms in concept 1:\")\n",
    "for t in idx_top_terms[:10]:\n",
    "    print(\" - %s : %0.02f\"%(all_terms[t], t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top terms in concept 1 appear related to accounting balance sheets; terms like \"net\", \"loss\", \"profit\".\n",
    "\n",
    "Now, back to our documents. Recall that `tfidf_lsa` is a transformation of our original tf-idf matrix from the term-space into a concept-space. The concept space is much more valuable, and we can use it to query most similar documents. We expect that two documents which about similar things should have similar vectors in `tfidf_lsa`. We can use a simple distance metric to measure the similarity, euclidean distance or cosine similarity being the two most common. \n",
    "\n",
    "Here, we'll select a single query document (index 300), calculate the distance of every other document to our query document, and take the one with the smallest distance to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY DOCUMENT:\n",
      " \n",
      "\r\n",
      "HUGHES TOOL <HT> UP ON MERGER SPECULATION\r\n",
      "\n",
      "    NEW YORK, March 6 - Hughes Tool Co rose one to 12-1/4 on\r\n",
      "1,658,000 shares, apparently reflecting a belief that Baker\r\n",
      "International Corp <BKO> will be able to persaude Hughes to go\r\n",
      "along with a previously announced merger, analysts said.\r\n",
      "    This week Hughes seemed to back out of the merger but then\r\n",
      "said it was still interested in talking.\r\n",
      "    \"It sounds like Baker wants it and if people are convinced\r\n",
      "a deal is going to go through the stock goes up,\" said analyst\r\n",
      "Phil Pace of Kidder, Peabody and Co. Holders of Hughes would\r\n",
      "get 0.8 share of Baker for each Hughes share. \r\n",
      "\r",
      " \n",
      "MOST SIMILAR DOCUMENT TO QUERY:\n",
      " \n",
      "\r\n",
      "HUGHES <HT> CHANGES STANCE ON MERGER AFTER SUIT\r\n",
      "\n",
      "<AUTHOR>    by Cal Mankowski, Reuters</AUTHOR>\r\n",
      "    NEW YORK, March 5 - A one billion dlr lawsuit pushed\r\n",
      "Hughes Tool Co into an about-face on its rejection of a\r\n",
      "proposed merger with Baker International Corp <BKO>, Wall\r\n",
      "Street analysts said.\r\n",
      "    Last night, Hughes said the planned merger with Baker was\r\n",
      "off. Baker then filed a suit seeking punitive damages from\r\n",
      "Hughes for calling off the merger. At midday today Hughes said\r\n",
      "it was still interested in the merger.\r\n",
      "    The analysts also said Hughes may be worried that its\r\n",
      "troubles could make it a takeover candidate.\r\n",
      "    There was speculation today that Harold Simmons, the Dallas\r\n",
      "investor, might try to acquire Hughes, but Simmons told Reuters\r\n",
      "he is not interested.\r\n",
      "    Simmons said he intends to file a 13-D with the Securities\r\n",
      "and Exchange Monday reporting a stake of five pct or more in\r\n",
      "some publicly traded company. He declined to identify the\r\n",
      "target other than to rule out Hughes.\r\n",
      "    One analyst said another factor in the latest Hughes\r\n",
      "turnabout was Borg-Warner Corp <BOR>, which owns 18.5 pct of\r\n",
      "Hughes. Borg-Warner ex-chairman J.F. Bere, who serves on the\r\n",
      "Hughes board, is believed to favor the merger with Baker.\r\n",
      "    Despite the Hughes statement that it is interested in a\r\n",
      "merger, and Baker's response that a merger is still possibile,\r\n",
      "analysts said no one could be certain where the situation was\r\n",
      "going.\r\n",
      "    \"I think the merger is not going through,\" said Phil Pace,\r\n",
      "analyst at Kidder, Peabody and Co. He said the merger \"lost a\r\n",
      "lot of its appeal\" when the U.S. Department of Justice required\r\n",
      "that Baker sell off its Reed Tool Co operation.\r\n",
      "    Although the Reed operation is relatively small in view of\r\n",
      "the total size of a combined Baker-Hughes, Pace said \"30 to 40\r\n",
      "pct of the cost savings are tied up in that.\"\r\n",
      "    \"They (Hughes) are obviously concerned about the lawsuit,\"\r\n",
      "said James Crandell, analyst at Salomon Brothers Inc.\r\n",
      "\"Apparently they are willing to continue discussions but\r\n",
      "whether they will alter their position, I don't know.\r\n",
      "    \"It's getting a little confusing,\" said James Carroll,\r\n",
      "analyst at PaineWebber Group Inc. He said the arguments cited\r\n",
      "by Hughes yesterday for not doing the merger \"tend to be weak.\"\r\n",
      "Hughes said yesterday that as a condition of the merger it\r\n",
      "wanted Reed Tool and other businesses sold prior to April 22,\r\n",
      "the projected merger date. A government decree allowed a longer\r\n",
      "period of time.\r\n",
      "    Hughes contended it was better to formally combine the\r\n",
      "companies with the status of Reed already settled. Baker\r\n",
      "apparently sees no reason to speed up the sale.\r\n",
      "    Carroll said Baker had previously estimated 110 to 130 mln\r\n",
      "dlrs in savings if the companies were combined without selling\r\n",
      "Reed. But he said Baker now thinks 75 to 85 mln dlrs will be\r\n",
      "saved while Hughes sees a saving of only 50 to 60 mln dlrs.\r\n",
      "    Carroll also noted that since the merger accord was first\r\n",
      "signed \"the outlook for the industry has improved materially.\"\r\n",
      "Hughes may simply feel the pressure on the oil service industry\r\n",
      "is lifting.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "query_idx = 300\n",
    "\n",
    "# take the concept representation of our query document\n",
    "query_features = tfidf_lsa[query_idx]\n",
    "\n",
    "# calculate the distance between query and every other document\n",
    "distances = [ distance.euclidean(query_features, feat) for feat in tfidf_lsa ]\n",
    "    \n",
    "# sort indices by distances, excluding the first one which is distance from query to itself (0)\n",
    "idx_closest = sorted(range(len(distances)), key=lambda k: distances[k])[1:]\n",
    "\n",
    "# print our results\n",
    "query_doc = docs[query_idx]\n",
    "return_doc = docs[idx_closest[0]]\n",
    "print(\"QUERY DOCUMENT:\\n %s \\nMOST SIMILAR DOCUMENT TO QUERY:\\n %s\" %(query_doc, return_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting find! Our query document appears to be about a pending merger for the company \"Hughes Tool (HT)\". Our return document is a related article about the same topic. Try looking at the next few closest results. A quick inspection reveals that most of them are about the same story.\n",
    "\n",
    "Thus we see the value of this procedure. It gives us a way to quickly identify articles which are related to each other. This can greatly aide journalists who have to sift through a lot of content which is not always indexed or organized usefully. \n",
    "\n",
    "More creatively, we can think of other ways this can be made useful. For example, what if instead of making our documents the articles themselves, what if they were made to be paragraphs from the articles? Then, we could potentially discover relevant paragraphs about one topic which are buried in an article which is otherwise about a different topic. We can combine this with handcrafted filters as well (date range, presence of a word or name, etc); perhaps you want to quickly find every quote politican X has made about topic Y. This provides an effective means to do so."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
