{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano XOR Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello world! Let's get started with some **deep learning**. This example is the bottom rung of the ladder to **deep learning**. To start understanding **deep learning**, as with many things, it is always a good idea to try to formulate a mental model of the problem. I would recommend taking a look at [this blog post](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) for an excellect explanation of the core concepts behind **deep learning**.\n",
    "\n",
    "![Hype train](images/hype_train.png)\n",
    "\n",
    "First, let's set up our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our input data `X` and output data `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]]\n",
      "y: [[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "X = theano.shared(value=np.asarray([[0, 1], [1, 0], [0, 0], [1, 1]]), name='X')\n",
    "y = theano.shared(value=np.asarray([[0], [0], [1], [1]]), name='y')\n",
    "print('X: {}\\ny: {}'.format(X.get_value(), y.get_value()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a Numpy random number generator and seed the built-in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rng = np.random.RandomState(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper method for generating the matrices (as Theano shared variables) for a single layer can be written as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(*shape):\n",
    "    assert len(shape) == 2\n",
    "    mag = 4. * np.sqrt(6. / sum(shape))\n",
    "    W_value = np.asarray(rng.uniform(low=-mag, high=mag, size=shape), dtype=theano.config.floatX)\n",
    "    b_value = np.asarray(np.zeros(shape[1], dtype=theano.config.floatX), dtype=theano.config.floatX)\n",
    "    W = theano.shared(value=W_value, name='W_{}'.format(shape), borrow=True, strict=False)\n",
    "    b = theano.shared(value=b_value, name='b_{}'.format(shape), borrow=True, strict=False)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this helper method, let's generate the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.28477995  0.90440604 -0.46122329  2.1135257   2.07365784]\n",
      " [-1.68430669 -1.65563108  2.23583464  3.39323698  2.78436792]]\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = layer(2, 5)\n",
    "W2, b2 = layer(5, 1)\n",
    "print(W1.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these weights, we can build our network. The hidden layer uses a `relu` activation function while the output layer uses a `sigmoid` activation function. From the outputs, we calculate the `cost` to minimize as the mean squared error between the calculated output and target output. Finally, we can get the network to start **deep learning** by applying our learning rule. As with many neural networks, we update each weight parameter in the direction that reduces the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = T.nnet.sigmoid(T.dot(T.nnet.relu(T.dot(X, W1) + b1), W2) + b2) # The whole network\n",
    "cost = T.mean((y - output) ** 2) # Mean squared error\n",
    "updates = [(p, p - 0.1 * T.grad(cost, p)) for p in [W1, W2, b1, b2]] # Subgradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's construct Theano functions for training and testing. The training function simply applies the learning updates, while the testing function outputs the `cost` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = theano.function(inputs=[], outputs=[], updates=updates)\n",
    "test = theano.function(inputs=[], outputs=cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's train and evaluate our network; lo and behold, it learns to separate the linearly nonseparable data points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost before: 0.475550048714\n",
      "Cost after: 0.000385103087629\n",
      "Time (s): 0.422646999359\n"
     ]
    }
   ],
   "source": [
    "print('Cost before:', test())\n",
    "start = time.time()\n",
    "for i in range(10000):\n",
    "    train()\n",
    "end = time.time()\n",
    "print('Cost after:', test())\n",
    "print('Time (s):', end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
